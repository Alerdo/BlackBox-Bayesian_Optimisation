{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hebo_overfitting.py\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import yeojohnson\n",
    "\n",
    "# Import your custom GP model here if needed:\n",
    "# from your_module import CustomKernelGPModel\n",
    "\n",
    "def check_overfitting(hebo_instance, test_x, test_y):\n",
    "    \"\"\"\n",
    "    Performs overfitting checks for small datasets. This function:\n",
    "      1. Compares Train vs Test LML\n",
    "      2. Performs Leave-One-Out (LOO) cross-validation on a small dataset\n",
    "      3. Checks average predictive variance (stddev)\n",
    "      4. Computes Log Predictive Density (LPD)\n",
    "      5. Computes MSE on the test set\n",
    "      6. Approximates BIC\n",
    "\n",
    "    :param hebo_instance: An instance of the HEBOOptimizer class, which has:\n",
    "       - hebo_instance.gp (trained GPyTorch model)\n",
    "       - hebo_instance.X, hebo_instance.y_raw, hebo_instance.y_transformed\n",
    "       - A method: hebo_instance.gp.get_lml() for Train LML\n",
    "    :param test_x: NumPy array of shape (N_test, D) for test inputs\n",
    "    :param test_y: NumPy array of shape (N_test,) or (N_test, 1) for test targets\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n=== Overfitting Checks ===\")\n",
    "\n",
    "    # Convert test data to tensors\n",
    "    test_x_t = torch.tensor(test_x, dtype=torch.float32)\n",
    "    test_y_t = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "    # 1) Train vs. Test LML\n",
    "    train_lml = hebo_instance.gp.get_lml()  # calls gp.get_lml()\n",
    "    hebo_instance.gp.eval()\n",
    "    hebo_instance.gp.likelihood.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_output = hebo_instance.gp(test_x_t)\n",
    "        test_lml_val = hebo_instance.gp.likelihood(test_output).log_prob(test_y_t).sum()\n",
    "\n",
    "    print(f\"Train LML: {train_lml:.4f} | Test LML: {test_lml_val.item():.4f}\")\n",
    "\n",
    "    # 2) Leave-One-Out (LOO) Cross-Validation\n",
    "    # Feasible only for very small data; it re-trains a mini-GP (slow for bigger sets).\n",
    "    n = len(hebo_instance.y_raw)\n",
    "    if n <= 20:  # arbitrary small threshold\n",
    "        loo_lmls = []\n",
    "        for i in range(n):\n",
    "            # Mask out the i-th point\n",
    "            mask = np.ones(n, dtype=bool)\n",
    "            mask[i] = False\n",
    "            x_loo = torch.tensor(hebo_instance.X[mask], dtype=torch.float32)\n",
    "            y_loo = torch.tensor(hebo_instance.y_transformed[mask], dtype=torch.float32)\n",
    "\n",
    "            # (Re)create a minimal GP from scratch\n",
    "            mini_gp = hebo_instance.__class__.gp.__new__(hebo_instance.gp.__class__)  # or directly instantiate your GP\n",
    "            mini_gp = hebo_instance.gp.__class__(x_loo, y_loo, hebo_instance.likelihood)\n",
    "\n",
    "            mini_gp.train()\n",
    "            opt = torch.optim.Adam(mini_gp.parameters(), lr=0.05)\n",
    "            mll_loo = gpytorch.mlls.ExactMarginalLogLikelihood(hebo_instance.likelihood, mini_gp)\n",
    "\n",
    "            # Quick short training loop\n",
    "            for _ in range(40):\n",
    "                opt.zero_grad()\n",
    "                out_loo = mini_gp(x_loo)\n",
    "                loss_loo = -mll_loo(out_loo, y_loo)\n",
    "                loss_loo.backward()\n",
    "                opt.step()\n",
    "\n",
    "            # Evaluate log probability for the left-out point\n",
    "            mini_gp.eval()\n",
    "            with torch.no_grad():\n",
    "                x_i = torch.tensor(hebo_instance.X[i:i+1], dtype=torch.float32)\n",
    "                y_i = torch.tensor([hebo_instance.y_transformed[i]], dtype=torch.float32)\n",
    "                out_i = mini_gp(x_i)\n",
    "                lml_i = mini_gp.likelihood(out_i).log_prob(y_i)\n",
    "                loo_lmls.append(lml_i.item())\n",
    "\n",
    "        avg_loo_lml = np.mean(loo_lmls)\n",
    "        print(f\"LOO Average LML (demo): {avg_loo_lml:.4f}\")\n",
    "    else:\n",
    "        print(\"Skipping LOO-CV (dataset bigger than 20).\")\n",
    "\n",
    "    # 3) Predictive Variance Check on test set\n",
    "    with torch.no_grad():\n",
    "        test_pred = hebo_instance.gp(test_x_t)\n",
    "        test_std = test_pred.stddev.numpy()\n",
    "    avg_std = np.mean(test_std)\n",
    "    print(f\"Avg Predictive Std (test set): {avg_std:.4f}\")\n",
    "\n",
    "    # 4) Log Predictive Density (LPD) on test set\n",
    "    lpd = test_lml_val.item() / len(test_y)\n",
    "    print(f\"Log Predictive Density (LPD) on test set: {lpd:.4f}\")\n",
    "\n",
    "    # 5) Mean Squared Error (MSE) on test set\n",
    "    mse_test = mean_squared_error(test_y, test_pred.mean.detach().numpy())\n",
    "    print(f\"Test MSE: {mse_test:.4f}\")\n",
    "\n",
    "    # 6) Bayesian Information Criterion (BIC) ~ k*log(n) - 2*logLik\n",
    "    # Here we use train_lml for logLik and approximate the number of parameters\n",
    "    n_params = sum(p.numel() for p in hebo_instance.gp.parameters() if p.requires_grad)\n",
    "    bic = n_params * np.log(n) - 2 * train_lml\n",
    "    print(f\"BIC (approx): {bic:.2f}\")\n",
    "\n",
    "    print(\"=== End Overfitting Checks ===\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
